# Data-Mining-Yelp-Dataset-Bussiness-User-Similarities

Step 1: In this step you will use the data from Step 1 to create a sparse table ğ‘… that holds the ratings. Load the pruned_data.csv file in the form of triads (user id, business id, rating). Then randomly remove 5% of the ratings from table ğ‘…. These are the ratings we want to predict so we will keep their position and values. The ğ‘… table with the ratings will no longer contain the values i removed.

Step 2: Implement the User-Based Collaborative Filtering (UCF) algorithm. The algorithm has a parameter ğ‘˜, which is the number of identical users it looks at. To calculate the value of a cell (ğ‘¢, ğ‘), calculate the total ğ‘ğ‘˜ (ğ‘¢, ğ‘) with ğ‘˜ most similar users who have rated the business ğ‘. We use the following equation for our prediction:

ğ‘(ğ‘¢, ğ‘) = (âˆ‘ğ‘¢â€²âˆˆğ‘ğ‘˜(ğ‘¢,ğ‘) ğ‘ (ğ‘¢, ğ‘¢â€²)ğ‘Ÿ(ğ‘¢â€², ğ‘))/ (âˆ‘ğ‘¢â€²âˆˆğ‘ğ‘˜(ğ‘¢,ğ‘) ğ‘ (ğ‘¢, ğ‘¢â€²))

In equation ğ‘  (ğ‘¢, â€² â€²) is the similarity between users ğ‘¢ and uâ€². I will use cosine similarity for my implementation.

The implementation has the following steps:
â€¢ Calculate the table of similarities between users
â€¢ For each pair (ğ‘¢, ğ‘) that i have removed:
    1. Find users who have rated ğ‘
    2. Take the similarity of these users with user u and keep ğ‘˜ more similar
    users
    3. Make two vectors, one with the similarities and one with the scores for the ğ‘˜ most
    like users
    4. Calculate the score with the above equation.

Step 3: Implement the Item-Based Collaborative Filtering (ICF) algorithm. The algorithm is essentially the same as in Step 2, i just work with the inverted array and exchange users and businesses. Below is its description for completeness:
The algorithm has a parameter ğ‘˜, which is the number of similar companies it will look at. To calculate the value of a cell (ğ‘¢, ğ‘) calculate the total ğ‘ğ‘˜ (ğ‘, ğ‘¢) with ğ‘˜'s most similar to ğ‘ companies that rated by ğ‘¢. Then use the following equation for your prediction:

ğ‘(ğ‘¢, ğ‘) = (âˆ‘ğ‘â€²âˆˆğ‘ğ‘˜(ğ‘,ğ‘¢) ğ‘ (ğ‘, ğ‘â€²)ğ‘Ÿ(ğ‘¢, ğ‘â€²)) /( âˆ‘ğ‘â€²âˆˆğ‘ğ‘˜(ğ‘,ğ‘¢) ğ‘ (ğ‘, ğ‘â€²))

In equation ğ‘  (ğ‘, â€² â€²) is the similarity between companies ğ‘ and bâ€². I will use cosine similarity for my implementation.
     
The implementation has the following steps:
â€¢ Calculate the table with similarities between companies 
â€¢ For each pair (ğ‘¢, ğ‘) that i have removed:
    5. Find the companies that  has rated u
    6. Take the similarity of these businesses with business ğ‘ and keep the ğ‘˜ longer
    similar companies
    7. Make two vectors, one with the similarities and one with the scores for the ğ‘˜ most similar
    businesses
    8. Calculate the score with the above equation. The calculation can be done with
    vector operations.

Step 4: Apply the Singular Value Decomposition (SVD) to the table ğ‘…, and hold the ğ‘˜ larger singular vectors to get a rank-ğ‘˜ table ğ‘…ğ‘˜. Then we use the value ğ‘ (ğ‘¢, ğ‘) = ğ‘…ğ‘˜ (ğ‘¢, ğ‘) for our prediction. If the value becomes less than 0, or greater than 5, round to 0 or 5.

Step 5: Evaluate our algorithms. For the evaluation i will use the RMSE (Root Mean Square Error) metric. If ğ‘Ÿi are the ratings we want to predict, and ğ‘i are the predictions of the algorithm, the RMSE of the algorithm is defined as:

ğ‘…ğ‘€ğ‘†ğ¸=âˆš (1/n âˆ‘(ğ‘Ÿiâˆ’ğ‘i)^2)
ğ‘–=1

I create graphs with RMSE for different k values for all algorithms. For the UCF algorithm we use the values [1,5,10,20,50,100,200,500,1000]. For the ICF algorithm we use the values [1,5,10,20,40,50,60,70,80,100]. For the SVD algorithm we use the values [1,5,10,20,30,40,50,75,100]. Also, we find the ğ‘˜ with the lowest error

I will also compare with the following simple "baselines":
    1. User Average (UA): Use the average ğ‘Ÿ (ğ‘¢) of the predictions ratings.
    Ì…Ì…Ì…Ì…Ì…Ì…
    2. Business Average (BA): Use the average ğ‘Ÿ (ğ‘) of the predictions ratings